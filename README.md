# Text-Image-Alignment Project

## Introduction

The `text-image-align` project aims to bridge the gap between textual descriptions and visual content. By leveraging the capabilities of the CLIP model, we've distilled its essence into a smaller model trained on approximately 350 images. This allows for efficient and accurate alignment between text and images, offering two primary functionalities:
1. Input a textual description and retrieve the most closely aligned image.
2. Input an image storage location and receive a language description that best describes the image.

## Features

- **CLIP Model Integration**: Harnesses the power of OpenAI's CLIP model to understand and align textual and visual content.
- **Model Distillation**: To ensure efficiency without a significant compromise on accuracy, the model has been distilled and trained on a curated dataset of around 350 images.
- **Bidirectional Functionality**:
  - **Text-to-Image**: Provide a textual description and get the most relevant image from the dataset.
  - **Image-to-Text**: Provide an image's storage location and receive a concise textual description.

## Prerequisites

Before you begin, ensure you have met the following requirements:

- Python 3.x
- Libraries: PIL, clip, pytorch, replicate

## Installation

1. Clone the repository:
\```bash
git clone https://github.com/shirou10086/text-image-align.git
\```

2. Navigate to the project directory:
\```bash
cd text-image-align
\```

3. Install the required packages:
\```bash
pip install Pillow clip torch
\```

## Usage

### Text-to-Image Retrieval

\```bash
python main.py
\```

### Image-to-Text Description

\```bash
python main.py
\```

## Data

The features extracted from the text and images are stored in the `features` folder. This ensures quick retrieval and description generation without the need to process the data repeatedly.

## Dataset

Our dataset consists of approximately 350 images curated to cover a broad spectrum of objects, scenes, and concepts. Each image in the dataset has been meticulously annotated with textual descriptions to train the distilled model. All the images are collected from Habitatsim and all the txt files are generated by i-t.py using img2prompt.
